{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "## import data processing/cleaning , data modeling libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import re as re\n",
    "import datetime as datetime\n",
    "import numpy as np\n",
    "import collections\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models import doc2vec\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import fasttext\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from gensim.models.wrappers import FastText\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "t0 = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outData43 = pd.DataFrame()\n",
    "limit = 20000\n",
    "\n",
    "outData43_text = pd.read_csv(\"reviews.txt\",encoding=\"utf-8\",header=None,names=[\"text\"])\n",
    "outData43_lab = pd.read_csv(\"labels.txt\",encoding=\"utf-8\",header=None,names=[\"label\"])\n",
    "#outData43_text = outData43_text[:limit]\n",
    "#outData43_lab = outData43_lab[:limit]\n",
    "#outData43[\"length\"]=outData43_text.text.apply(lambda x:len(x))\n",
    "# outData43[\"label\"]=outData43_lab.label\n",
    "#print(outData43.length)\n",
    "# print(outData43.head(n=2))\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(outData43_text,outData43_lab,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cruz',\n",
       " 'salim',\n",
       " 'india',\n",
       " 'title',\n",
       " 'marianne',\n",
       " 'chak',\n",
       " 'rukh',\n",
       " 'khan',\n",
       " 'song',\n",
       " 'singh',\n",
       " 'full',\n",
       " 'shah',\n",
       " 'sukhvinder']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## NLP PRE-PROCESSING\n",
    "\n",
    "#text=\" the JanaSena Party Formation Day celebrations || LIVE funny celebration|| Pawan Kalyan || Guntur\"\n",
    "text=\"Chak De India | Full Title Song | Shah Rukh Khan | Sukhvinder Singh | Salim | Marianne D'Cruz\"\n",
    "\n",
    "def nltk_clean_sent(line):\n",
    "    if len(line)>0:\n",
    "        ## remove the punctuation/emoticons/digits/multispaces with single from the line\n",
    "        ## dont make lowercase before the pos tagging\n",
    "        line_lower = line.strip()\n",
    "        line_punct = re.sub('['+string.punctuation+']',' ',line_lower)\n",
    "        line_emots = re.sub(r'[\\u200b-\\u2fff]+',' ',line_punct)\n",
    "        line_digis = re.sub(r'[0-9]+',' ',line_emots)\n",
    "        line_spaces = re.sub(r'[\\s]+',' ',line_digis)\n",
    "        line = line_spaces\n",
    "    return line.lower()\n",
    "\n",
    "def nltk_extract_postags(tokens):\n",
    "    cleaned_str = ''\n",
    "    #tokens = nltk.word_tokenize(line)\n",
    "    tokens_pos = PerceptronTagger().tag(tokens)\n",
    "    #print(tokens_pos)\n",
    "    # noun tags\n",
    "    nn_tags = ['NN', 'NNP', 'NNP', 'NNPS', 'NNS']\n",
    "    # adjectives\n",
    "    jj_tags = ['JJ', 'JJR', 'JJS']\n",
    "    # verbs\n",
    "    vb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "    nltk_tags = nn_tags + jj_tags + vb_tags\n",
    "    \n",
    "    for tag_word in tokens_pos:\n",
    "#         if tag_word[1] in nltk_tags:\n",
    "            if len(tag_word[0])>2:\n",
    "                cleaned_str += tag_word[0].lower()+tag_word[1]+' '\n",
    "    return cleaned_str.strip().split()\n",
    "\n",
    "def nltk_apply_lemma(line):\n",
    "    ## lemma is fine rather than stemming\n",
    "    tokens_lemmas = [WordNetLemmatizer().lemmatize(word) for word in line.split()]\n",
    "    #tokens_lemmas = [PorterStemmer().stem(word) for word in line.split()]\n",
    "    ## stemming\n",
    "    tokens_stops = [word for word in tokens_lemmas if word not in stopwords.words('english')]\n",
    "    tokens_stops = [word for word in tokens_stops if len(word.strip())>2]\n",
    "    tokens_stops = list(set(tokens_stops))\n",
    "    return tokens_stops\n",
    "\n",
    "def nltk_extract_tags(line):\n",
    "        ## tokenize the sentence/get tokens that contains only letters\n",
    "        line_clean = nltk_clean_sent(line)\n",
    "        ## apply lemmatize/stemming and remove stopwords\n",
    "        token_lemma = nltk_apply_lemma(line_clean)\n",
    "#         apply postags to the words and get only couple of tags and word length >2\n",
    "#         tokens_pos = nltk_extract_postags(token_lemma)\n",
    "        \n",
    "        return token_lemma\n",
    "    \n",
    "nltk_extract_tags(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  \\\n",
      "4726  the story is somewhat stilted  what with the m...   \n",
      "8368  based on the idea from gackt  moon child took ...   \n",
      "\n",
      "                                      clean_title_words  \\\n",
      "4726  [main, memorable, sally, writer, ever, compete...   \n",
      "8368  [main, become, considering, stricken, order, m...   \n",
      "\n",
      "                                       clean_title_text  \n",
      "4726  main memorable sally writer ever compete affec...  \n",
      "8368  main become considering stricken order made gr...  \n"
     ]
    }
   ],
   "source": [
    "# Compute clean title\n",
    "x_train['clean_title_words'] = x_train.text.apply(nltk_extract_tags)\n",
    "x_test['clean_title_words'] = x_test.text.apply(nltk_extract_tags)\n",
    "#print(df_en.head(n=2))\n",
    "x_train['clean_title_text'] = x_train.clean_title_words.apply(lambda x:' '.join(x))\n",
    "x_test['clean_title_text'] = x_test.clean_title_words.apply(lambda x:' '.join(x))\n",
    "print(x_train.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['brown', 'performance', 'year', 'give', 'dream', 'case', 'awesome', 'fricker', 'find', 'movie', 'daniel', 'much', 'forty', 'palsey', 'someone', 'reason', 'christie', 'day', 'severe', 'lived', 'cerebral', 'inspirational', 'brenda', 'lewis', 'achieve']]\n",
      "[ -1.52037768e-02  -1.73712745e-01   8.37332606e-02  -1.06364302e-01\n",
      "   4.58639525e-02  -4.84243445e-02  -1.76045910e-01  -2.00325564e-01\n",
      "   3.11818480e-01   8.02127831e-03   2.00823903e-01  -2.56282508e-01\n",
      "   1.08550526e-01   1.03271924e-01   1.45190060e-01  -3.85168642e-02\n",
      "   3.04789603e-01  -1.47695690e-02   4.89773974e-02   7.04371631e-02\n",
      "  -5.80405928e-02  -1.17852710e-01   1.46775588e-01   1.28019154e-01\n",
      "   1.15412660e-01   4.31235224e-01   2.73393095e-02  -1.29886091e-01\n",
      "   7.83788934e-02  -3.56101662e-01   1.11137748e-01   1.43128023e-01\n",
      "   3.12396973e-01  -3.22554976e-01   2.78450698e-01   3.15884918e-01\n",
      "  -1.81622338e-02   4.52530151e-03   9.94418189e-02   3.94219905e-01\n",
      "  -3.57410114e-04  -4.24078256e-02   1.84804127e-01   1.57931805e-01\n",
      "   6.74293637e-02  -2.16713980e-01  -5.01233697e-01   9.97374728e-02\n",
      "  -1.61615804e-01  -3.07540745e-02]\n"
     ]
    }
   ],
   "source": [
    "# ## DOC2VEC model\n",
    "#finaldoc = list(x_train['clean_title_words'])+list(x_test['clean_title_words'])\n",
    "cleaneddoc = list(x_train['clean_title_words'])\n",
    "cleaneddoc1 = list(x_test['clean_title_words'])\n",
    "print(cleaneddoc[:1])\n",
    "taggeddoc = [doc2vec.TaggedDocument(val,[idx]) for idx,val in enumerate(cleaneddoc)]\n",
    "\n",
    "d2vmodel = doc2vec.Doc2Vec(size=50,min_count=2,epochs=10)\n",
    "d2vmodel.build_vocab(taggeddoc)\n",
    "# #print(d2vmodel[0])\n",
    "d2vmodel.train(taggeddoc,total_examples=d2vmodel.corpus_count,epochs=d2vmodel.epochs)\n",
    "\n",
    "xtrain = [d2vmodel.infer_vector(val) for idx,val in enumerate(cleaneddoc)]\n",
    "xtest = [d2vmodel.infer_vector(val) for idx,val in enumerate(cleaneddoc1)]\n",
    "print(xtest[0])\n",
    "\n",
    "##feature_vecto = d2vmodel.docvecs.doctag_syn0\n",
    "##print(len(d2vmodel_vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "5000\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy score= 0.834\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.82      0.85      0.83      2458\n",
      "   positive       0.85      0.82      0.83      2542\n",
      "\n",
      "avg / total       0.83      0.83      0.83      5000\n",
      "\n",
      "TIME taken for Base Modeling (LinearSVC(C=10, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.1,\n",
      "     verbose=0) with Universal Sentence Encoder)= 0:00:04.955837\n",
      "training accuracy score= 0.826\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.82      0.83      0.83      2490\n",
      "   positive       0.83      0.82      0.83      2510\n",
      "\n",
      "avg / total       0.83      0.83      0.83      5000\n",
      "\n",
      "TIME taken for Base Modeling (SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=50, n_iter=None,\n",
      "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
      "       shuffle=True, tol=None, verbose=0, warm_start=False) with Universal Sentence Encoder)= 0:00:06.465512\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
